{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use the exact same tools we use in our Neural Network class, except for the Python Gym. The Gym is a module developed by the OpenAI, it lets us use game environments for training the reinforcement algorithm. The Gym supplies us with State (environment), rewards and actions for training our ANN. Particularly, We will be working with the Q-learning Algorithm. We will be using traning network for this demo and exclude the traning expect of since it take more than a day to complete the training, instead we will focus on how to run our train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# OpenAI Gym\n",
    "import gym_tetris as gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Rendering tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we note that the state given by 'observation' is a 3-tensor with 430x330 dimension and 3 for color. We will use this state as input and use a convolutional neural network to detect pieces in the screen, which renders us with millions of parameters making the network extremely slow and difficult to train. To resolve this, we will change transform it to grey image and downsize it to 10x20x3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Tetris-v0')\n",
    "downsize = (10,20)\n",
    "observation=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To downsize the observation, we use cv2 module. To make it undemanding, we have build this function that downsizes the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img,downsize):\n",
    "    resized = cv2.resize(img,\n",
    "                         dsize=downsize,\n",
    "                         interpolation=cv2.INTER_CUBIC)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resized_observation = resize(observation[17:423,7:212],downsize)\n",
    "in_size = resized_observation.shape\n",
    "obs_size = resized_observation.size\n",
    "action_space = 6\n",
    "display(in_size)\n",
    "display(obs_size)\n",
    "display(action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use convolutional neural network for our model. The input is (20, 10, 3) tensor which is the state of the game. Convolutional NN is best for this problem because Convolutional NN is optimal for detecting pieces. Our network follows a very conventional method which is convolution by 3x3 kernal, convolution, maxpool and dense. Then we use regular fully connected NN going from 64 to 20. This model outputs array of length 6. Output represents the Q values that we learned in the class. We will choose the maximum Q value for the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1, 1, 64)          38464     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1, 64)          4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 1, 64)          4160      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 48,210\n",
      "Trainable params: 48,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_model(state, action_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(20, 10), activation='relu',\n",
    "                                    input_shape=[state.shape[0],\n",
    "                                                 state.shape[1],\n",
    "                                                 state.shape[2]]))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "    return model\n",
    "\n",
    "model = make_model(resize(observation,downsize), \n",
    "                   action_space)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key aspect of the Q-learning is the Replay Memory. Replay Memory stores the state, action and the reward associated with that action. Firstly, we initialize the class ReplayMemory using __init__ function. We store state in the self.current_state array which is of the same dimension as our observation. Actions and rewards are stored in self.action and self.reward arrays respective.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size, state_size, action_size):\n",
    "        self.state_size = [state_size.shape[0], state_size.shape[1], state_size.shape[2]]\n",
    "        self.action_size = action_size\n",
    "        self.size = 0\n",
    "        self.maxsize = memory_size\n",
    "        self.current_index = 0\n",
    "        self.current_state = np.zeros([memory_size, \n",
    "                                       self.state_size[0], \n",
    "                                       self.state_size[1], \n",
    "                                       self.state_size[2]])\n",
    "        self.action = [0]*memory_size # Remember, actions are integers...\n",
    "        self.reward = np.zeros([memory_size])\n",
    "        self.next_state = np.zeros([memory_size, \n",
    "                                    self.state_size[0], \n",
    "                                    self.state_size[1],\n",
    "                                    self.state_size[2]])\n",
    "        self.done = [False]*memory_size # Boolean (terminal transition?)\n",
    "        \n",
    "    def remember(self, current_state, action, reward, next_state, done):\n",
    "    # Stores a single memory item\n",
    "        self.current_state[self.current_index,:] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.next_state[self.current_index,:] = next_state\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index+1)%self.maxsize\n",
    "        self.size = max(self.current_index,self.size)\n",
    "    \n",
    "    def replay(self, model, target_model, num_samples, sample_size, gamma):\n",
    "        # Run replay!\n",
    "        \n",
    "        # Can't train if we don't yet have enough samples to begin with...\n",
    "        if self.size < sample_size:\n",
    "            return\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Select sample_size memory indices from the whole set\n",
    "            current_sample = np.random.choice(self.size,sample_size,replace=False)\n",
    "            \n",
    "            # Slice memory into training sample\n",
    "            current_state = self.current_state[current_sample,:]\n",
    "            action = [self.action[j] for j in current_sample]\n",
    "            reward = self.reward[current_sample]\n",
    "            next_state = self.next_state[current_sample,:]\n",
    "            done = [self.done[j] for j in current_sample]\n",
    "            model_targets = model.predict(current_state)\n",
    "            # Create targets from argmax(Q(s+1,a+1))\n",
    "            # Use the target model!\n",
    "            targets = reward + gamma*np.amax(target_model.predict(next_state),axis=1)\n",
    "\n",
    "            # Absorb the reward on terminal state-action transitions\n",
    "            targets[done] = reward[done]\n",
    "            # Update just the relevant parts of the model_target vector...\n",
    "            model_targets[range(sample_size),action] = targets\n",
    "\n",
    "            # Update the weights accordingly\n",
    "            model.fit(current_state,model_targets,epochs=1,verbose=0,batch_size=sample_size)\n",
    "\n",
    "        # Once we have finished training, update the target model\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.95\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.2\n",
    "episodes = 30000\n",
    "\n",
    "\n",
    "replay_iterations = 50\n",
    "replay_sample_size = 1024\n",
    "\n",
    "# Peformance stats\n",
    "times_window = deque(maxlen=100)\n",
    "mean_times = deque(maxlen=episodes)\n",
    "rewards = deque(maxlen=episodes)\n",
    "epsilons = deque(maxlen=episodes)\n",
    "max_Qs = deque(maxlen=episodes)\n",
    "min_Qs = deque(maxlen=episodes)\n",
    "avg_Qs = deque(maxlen=episodes)\n",
    "\n",
    "# Initialize the environment and agent data structures\n",
    "env = gym.make('Tetris-v0')\n",
    "observation = env.reset()\n",
    "resized_obs = resize(observation[17:423,7:212],downsize)\n",
    "model = make_model(resized_obs, action_space)\n",
    "target_model = make_model(resized_obs, action_space)\n",
    "memory = ReplayMemory(10000, resized_obs, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the code, you will need to download the weight from the our github website. https://github.com/CSCI4850/S19-team5-project/blob/master/weights.h5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run the model. We start by initaiting the gym. Observation give you the state of the game that we will feed into the network. We will let time and reward to equal to zero. We will have to resize the observation for each instance. plt.imsho yeilds the downsized state. Finally, we use the max Q value that we aquire from our model. Q value determines the action of the tetris. <br>\n",
    "\n",
    "You can also uncomment the display(plt.gcf()) to watch the model play tetris in the real time. However, ther is a chance of getting display error which we do not know how to fix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test out the agent!\n",
    "# Initialize the environment and let the agent decide how to act!\n",
    "env.close()\n",
    "env = gym.make('Tetris-v0')\n",
    "observation = env.reset()\n",
    "done = False\n",
    "time = 0\n",
    "reward = 0.0\n",
    "while not done:\n",
    "    if not time%3:\n",
    "        obs = resize(observation[17:423,7:212],downsize)\n",
    "        plt.imshow(obs)\n",
    "        plt.title(\"%s | Time: %d | Reward: %f\" % (env.spec.id, time, reward))\n",
    "        clear_output(wait=True)\n",
    "        Q = model.predict(np.expand_dims(obs,axis=0))\n",
    "        action = np.argmax(Q)\n",
    "        #display(plt.gcf())\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        next_height=info['height']\n",
    "        if (done):\n",
    "            break\n",
    "    else:\n",
    "        action=0\n",
    "        observation, reward, done, info = env.step(action)\n",
    "    time+=1\n",
    "clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
