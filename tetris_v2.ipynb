{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Gym\n",
    "import gym_tetris as gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Rendering tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Tetris-v0')\n",
    "downsize = (26,51)\n",
    "dsize = (51,26)\n",
    "observation=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img,downsize):\n",
    "    resized = cv2.resize(img,\n",
    "                         dsize=downsize,\n",
    "                         interpolation=cv2.INTER_CUBIC)\n",
    "    bwarray=[]\n",
    "    for x in range(resized.shape[0]):\n",
    "        row=[]\n",
    "        for y in range(resized.shape[1]):\n",
    "            r,g,b = observation[x][y]\n",
    "            if int(r) > 0 or int(g) > 0 or int(b) > 0:\n",
    "                row.append([1])\n",
    "            else:\n",
    "                row.append([0])\n",
    "        bwarray.append(row)\n",
    "    bwarray=np.array(bwarray)\n",
    "    return bwarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 26, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resized_observation = resize(observation[17:423,7:212],downsize)\n",
    "in_size = resized_observation.shape\n",
    "obs_size = resized_observation.size\n",
    "display(in_size)\n",
    "display(obs_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 44, 19, 128)       8320      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 12, 128)       1048704   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 18, 6, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 6, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 13824)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                276500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                252       \n",
      "=================================================================\n",
      "Total params: 1,333,776\n",
      "Trainable params: 1,333,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_model(state, action_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(128, kernel_size=(8, 8), activation='relu',\n",
    "                                    input_shape=[state.shape[0],\n",
    "                                                 state.shape[1],\n",
    "                                                 state.shape[2]]))\n",
    "    model.add(keras.layers.Conv2D(128, (8, 8), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "    return model\n",
    "\n",
    "#model = make_model(resized_observation, env.action_space.n)\n",
    "#model.summary()\n",
    "model = make_model(resize(observation,downsize), \n",
    "                   env.action_space.n)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size, state_size, action_size):\n",
    "        self.state_size = [state_size.shape[0], state_size.shape[1], state_size.shape[2]]\n",
    "        self.action_size = action_size\n",
    "        self.size = 0\n",
    "        self.maxsize = memory_size\n",
    "        self.current_index = 0\n",
    "        #self.current_state = np.zeros([memory_size, self.state_size[0], self.state_size[1], \n",
    "        #                               self.state_size[2]])\n",
    "        self.current_state = np.zeros([memory_size, \n",
    "                                       self.state_size[0], \n",
    "                                       self.state_size[1], \n",
    "                                       self.state_size[2]])\n",
    "        self.action = [0]*memory_size # Remember, actions are integers...\n",
    "        self.reward = np.zeros([memory_size])\n",
    "        self.next_state = np.zeros([memory_size, \n",
    "                                    self.state_size[0], \n",
    "                                    self.state_size[1],\n",
    "                                    self.state_size[2]])\n",
    "        self.done = [False]*memory_size # Boolean (terminal transition?)\n",
    "        \n",
    "    def remember(self, current_state, action, reward, next_state, done):\n",
    "        # Stores a single memory item\n",
    "        self.current_state[self.current_index,:] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.next_state[self.current_index,:] = next_state\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index+1)%self.maxsize\n",
    "        self.size = max(self.current_index,self.size)\n",
    "    \n",
    "    def replay(self, model, target_model, num_samples, sample_size, gamma):\n",
    "        # Run replay!\n",
    "        \n",
    "        # Can't train if we don't yet have enough samples to begin with...\n",
    "        if self.size < sample_size:\n",
    "            return\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Select sample_size memory indices from the whole set\n",
    "            current_sample = np.random.choice(self.size,sample_size,replace=False)\n",
    "            \n",
    "            # Slice memory into training sample\n",
    "            current_state = self.current_state[current_sample,:]\n",
    "            action = [self.action[j] for j in current_sample]\n",
    "            reward = self.reward[current_sample]\n",
    "            next_state = self.next_state[current_sample,:]\n",
    "            done = [self.done[j] for j in current_sample]\n",
    "            if current_state.size == obs_size:\n",
    "                model_targets = model.predict(resize(current_state, dsize))\n",
    "                # Create targets from argmax(Q(s+1,a+1))\n",
    "                # Use the target model!\n",
    "                print(next_state.size)\n",
    "                targets = reward + gamma*np.amax(target_model.predict(next_state,axis=1))\n",
    "\n",
    "                # Absorb the reward on terminal state-action transitions\n",
    "                targets[done] = reward[done]\n",
    "                # Update just the relevant parts of the model_target vector...\n",
    "                model_targets[range(sample_size),action] = targets\n",
    "\n",
    "                # Update the weights accordingly\n",
    "                model.fit(resize(current_state, downsize),model_targets,\n",
    "                          epochs=1,verbose=0,batch_size=sample_size)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Once we have finished training, update the target model\n",
    "            target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.994\n",
    "epsilon_min = 0.01\n",
    "episodes = 3000\n",
    "\n",
    "replay_iterations = 100\n",
    "replay_sample_size = 256\n",
    "\n",
    "# Peformance stats\n",
    "times_window = deque(maxlen=100)\n",
    "mean_times = deque(maxlen=episodes)\n",
    "\n",
    "# Initialize the environment and agent data structures\n",
    "env = gym.make('Tetris-v0')\n",
    "model = make_model(resize(env.reset(),dsize), env.action_space.n)\n",
    "target_model = make_model(resize(env.reset(),dsize), env.action_space.n)\n",
    "memory = ReplayMemory(8000, resize(env.reset(),dsize), env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this when display surface quit\n",
    "env = gym.make('Tetris-v0')\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40/3000 - time: 932, mean-time: 893, epsilon: 0.786059, reward: 16.7924"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "mean_time = 900\n",
    "# Perform the training!\n",
    "for episode in range(episodes):\n",
    "    current_state = env.reset()\n",
    "    current_state = current_state[17:423,7:212]\n",
    "    reward_earned = 0\n",
    "    time = 0\n",
    "    while not done:\n",
    "        time += 1\n",
    "        Q = model.predict(np.expand_dims(resize(current_state, dsize), axis=0)) # Compute Q\n",
    "        action = np.random.choice(env.action_space.n) if np.random.random() < epsilon else np.argmax(Q)\n",
    "        next_state, reward, done, info = env.step(action) # take action!\n",
    "        next_state = resize(next_state[17:423,7:212], dsize)\n",
    "        if done:\n",
    "            reward -= 10.0\n",
    "            reward_earned -= 10.0\n",
    "            if time < mean_time:\n",
    "                reward -= 20.0\n",
    "                reward_earned -= 20.0\n",
    "            else:\n",
    "                reward += 10.0\n",
    "                reward_earned += 10.0\n",
    "        reward += 0.01*pow(time,0.1)\n",
    "        reward_earned += 0.01*pow(time,0.1)\n",
    "        if current_state.size == obs_size:\n",
    "            resize_state = resize(current_state,dsize)\n",
    "            memory.remember(resize_state, action,reward,next_state,done) # Store in memory...\n",
    "        current_state = next_state # Transition to next state!\n",
    "    epsilon = epsilon * epsilon_decay if epsilon > epsilon_min else epsilon_min\n",
    "    times_window.append(time)\n",
    "    mean_time = np.mean(times_window)\n",
    "    mean_times.append(mean_time)\n",
    "    print('\\rEpisode %d/%d - time: %d, mean-time: %d, epsilon: %f, reward: %g'%(episode+1,episodes,time,mean_time,epsilon,reward_earned),end='')\n",
    "    done = False\n",
    "    \n",
    "    # Training...\n",
    "    memory.replay(model,target_model,replay_iterations,replay_sample_size,gamma)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_times)\n",
    "plt.title(\"Tetris Q-Learning Performance\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Life Span\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
