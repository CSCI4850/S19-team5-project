{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nhz04\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Gym\n",
    "import gym_tetris as gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "\n",
    "# Rendering tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 82, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAAEICAYAAAAuvnqCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD3RJREFUeJzt3XuQlfV9x/H3Bxa0Yh3AVIsgipZ4aWYSHWy8tWEkJl6jM5XGS1q0Kv80EdOkRtNpU5t2Jk5TL0nbdCjGEm2UFG0weCs1ikk7IS5aExEJ1gusrqKCtxgzUr794/ltetgc2N3zPct5dvm8Zs6cfX7nuXzPw/ns8/zOPjw/RQRm1poxnS7AbCRzgMwSHCCzBAfILMEBMktwgMwSHKDdiKR7JM1r8zr/QtIt7VznSOIAtUjSs5JekjShoe0SSQ/ugu3+TNJbDY+/G8yyEXFqRCwezvp2Nw5QThewoAPbPTMi9m54fLIDNRgOUNbfAJ+VNLHZi5IOl7RC0mZJ6yT9XmmfIek1SWPK9CJJmxqWu0XS5UMtRtKFkv5T0lclvS7pSUlzGl5/UNIl5effkLSyzPeKpCUN8x0v6eHy2sOSjm94bUZZ7k1JK4D39KvhWEn/Vd7fY5JmD/V9jCQOUE438CDw2f4vlFO7FcA3gf2A84B/kPSbEfEM8AZwVJn9t4G3JB1Rpn8HWNliTR8Enqb6YH8BuEPS5CbzfRH4d2ASMA34aql7MnAX8BVgX+Ba4C5J+5blvgmsLuv/IvCLPpWkqWXZvwImU+2X2yX9WovvpfYcoLw/Bz7V5ENyBvBsRNwUEVsj4hHgduCc8vpK4EOSfr1MLy3TM4B9gMd2ss1vl9/wfY9LG17bBFwfEe9GxBJgHXB6k3W8CxwEHBAR70TE90v76cD6iLi51H0r8CRwpqTpwDHAn0XEzyPiIeA7Dev8BHB3RNwdEdsiYgXVL5nTdvJeRjQHKCkiHgeWA1f2e+kg4IONH3TgAqAvMCuB2VRHm4eojmQfKo/vRcS2nWz27IiY2PD4p4bXno/trxB+DjigyTquAAT8UNIaSX9Y2g8oyzR6DphaXtsSET/t91rje57b7z2fCEzZyXsZ0bo6XcAo8QXgEeBvG9o2Aisj4uQdLLOSqg/VU37+PvCPwDu0fvoGMFWSGkI0Hbiz/0wR8SJwKYCkE4H/kPQQ8AJVEBpNB+4FeoFJkiY0hGg60LetjcDNEXEpuwkfgdogIp4ClgCXNTQvB94r6fcljSuPY/r6ORGxHvgZ1WnPQxHxBvAS8LvkArQfcFnZ3lzgCODu/jNJmitpWpncQhWC/y3zvlfS+ZK6JH0cOBJYHhHPUZ2SXS1pfAnemQ2rvYXqVO+jksZK2lPS7IbtjDoOUPv8JfCLvwlFxJvAR4BzqX6rvwhcA+zRsMxK4NWI2NAwLeDRAbb1nX5/B/q3htdWATOBV4C/Bs6JiFebrOMYYJWkt6iOUAsi4pky7xnAZ4BXqU71zoiIV8py51N9UbGZ6sj7jYb3vBE4C/g88DLVEelPGMWfM/k/1I0eki4ELomIEztdy+5i1P5mMNsVHCCzhGE5hZN0CnADMBZYFBFfavtGzGqg7QGSNBb4CXAy1Ve0DwPnRcQTbd2QWQ0Mx9+Bfgt4KiKeBpB0G9U3MzsMUFdXV4wbN4533nlnGMoxa01EaKB5hqMPNJXq68s+PaVtO5LmS+qW1D1mzBgOPfRQxowZw5gx7pbZyDEcR6Bmqf2l88SIWAgsBJAUa9as4fTTq0u27rnnHgC2bdvZ1SxmnTccv+57gAMbpqdR/SHRbNQZji8Ruqi+RJgDPE/1JcL5EbFmJ8sEwF133QXA22+/DcDcuXPbWpvZUAymD9T2U7iI2Crpk8B9VF9jf31n4TEbyYblauyIuJsmFzAOZNOm6j9lXnTRRe0uyWxY+Csvs4RaXEza1wcyq5NO/R3IbLfhAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbQcoAkHSjpAUlrJa2RtKC0T5a0QtL68jypfeWa1UvLw5tImgJMiYhHJP0qsBo4G7gQ2BwRX5J0JTApIj43wLo8vInVzrAObxIRvRHxSPn5TWAt1XD2ZwGLy2yLqUJlNiq1pQ8k6WDgKGAVsH9E9EIVMmC/dmzDrI7SY6RK2hu4Hbg8It6QBjzq9S03H5if3b5ZJ6WGeJQ0DlgO3BcR15a2dcDsiOgt/aQHI+KwAdbjPpDVzrD2gVQdam4E1vaFp7gTmFd+ngcsa3UbZnWX+RbuROB7wI+BbaX581T9oG8B04ENwNyI2DzAunwEstoZzBHIo3Sb7YBH6TYbZg6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklpAMkaaykRyUtL9MzJK2StF7SEknj82Wa1VM7jkALgLUN09cA10XETGALcHEbtmFWS6kASZoGnA4sKtMCTgKWllkWA2dntmFWZ9kj0PXAFfz/IMP7Aq9FxNYy3QNMbbagpPmSuiV1J2sw65jMMPdnAJsiYnVjc5NZmw4gHBELI2JWRMxqtQazTutKLHsC8DFJpwF7AvtQHZEmSuoqR6FpwAv5Ms3qqeUjUERcFRHTIuJg4FzguxFxAfAAcE6ZbR6wLF2lWU0Nx9+BPgf8saSnqPpENw7DNsxqQRFNuyi7tgip80WY9RMRzfr02/GVCGYJDpBZggNkluAAmSU4QGYJDpBZggNkluAAmSU4QGYJDpBZggNkluAAmSU4QGYJDpBZggNkluAAmSU4QGYJDpBZggNkluAAmSU4QGYJDpBZggNkluAAmSU4QGYJDpBZggNkluAAmSU4QGYJDpBZggNklpAdpXuipKWSnpS0VtJxkiZLWiFpfXme1K5izeomewS6Abg3Ig4H3g+sBa4E7o+ImcD9ZdpsVGp5hDpJ+wCPAYdEw0okrQNmR0SvpCnAgxFx2ADr8gh1VjvDPULdIcDLwE2SHpW0SNIEYP+I6C0F9AL7NVtY0nxJ3ZK6EzWYdVTmCDQL+AFwQkSsknQD8AbwqYiY2DDflojYaT/IRyCro+E+AvUAPRGxqkwvBY4GXiqnbpTnTYltmNVaywGKiBeBjZL6+jdzgCeAO4F5pW0esCxVoVmNpYa5l/QBYBEwHngauIgqlN8CpgMbgLkRsXmA9fgUzmpnMKdwqQC1iwNkdTTcfSCz3Z4DZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklpAIk6dOS1kh6XNKtkvaUNEPSKknrJS2RNL5dxZrVTcsBkjQVuAyYFRHvA8YC5wLXANdFxExgC3BxOwo1q6PsKVwX8CuSuoC9gF7gJKoBhwEWA2cnt2FWW5lBhp8Hvkw1Dmov8DqwGngtIraW2XqAqc2WlzRfUrek7lZrMOu0zCncJOAsYAZwADABOLXJrE3HP42IhRExKyJmtVqDWadlTuE+DDwTES9HxLvAHcDxwMRySgcwDXghWaNZbWUCtAE4VtJekgTMAZ4AHgDOKfPMA5blSjSrr9Qw95KuBj4ObAUeBS6h6vPcBkwubZ+IiJ8PsB4Pc2+1M5hh7lMBahcHyOpoMAHylQhmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklOEBmCQ6QWYIDZJbgAJklDBggSV+XtEnS4w1tkyWtkLS+PE8q7ZL0FUlPSfqRpKOHs3izThvMEeifgVP6tV0J3B8RM4H7yzRUo3TPLI/5wNfaU6ZZPQ0YoIh4CNjcr/ksYHH5eTFwdkP7N6LyA6oRu6e0q1izumm1D7R/RPQClOf9SvtUYGPDfD2l7ZdImi+pW1J3izWYdVxXm9fXbFDWpgMIR8RCYCF4kGEbuVo9Ar3Ud2pWnjeV9h7gwIb5pgEvtF6eWb21GqA7gXnl53nAsob2Pyjfxh0LvN53qmc2KkXETh/ArUAv8C7VEeZiYF+qb9/Wl+fJZV4Bfw/8D/BjYNZA6y/LhR9+1O0xmM+uyge4o9wHsjqKiGZ9+u34SgSzBAfILMEBMktwgMwSHCCzBAfILMEBMktwgMwSHCCzhHZfjd2qV4Cflue6eg/1rg/qX+NIqu+gwSxQi0t5ACR1R8SsTtexI3WvD+pf42isz6dwZgkOkFlCnQK0sNMFDKDu9UH9axx19dWmD2Q2EtXpCGQ24jhAZgm1CJCkUyStK3c0vXLgJYa9ngMlPSBpraQ1khaU9qZ3ZO1gnWMlPSppeZmeIWlVqW+JpPEdrG2ipKWSniz78bga7r9Pl3/fxyXdKmnPoe7DjgdI0liq+yicChwJnCfpyM5WxVbgMxFxBHAs8Eelph3dkbVTFgBrG6avAa4r9W2hun9Fp9wA3BsRhwPvp6qzNvtP0lTgMqr7drwPGAucy1D34WBunDCcD+A44L6G6auAqzpdV78alwEnA+uAKaVtCrCugzVNo/oQngQsp7qhyytAV7P9uotr2wd4hvIlVUN7nfZf301AJ1NdkbMc+OhQ92HHj0AM4W6mnSDpYOAoYBU7viNrJ1wPXAFsK9P7Aq9FxNYy3cn9eAjwMnBTOcVcJGkCNdp/EfE88GVgA9Vdp14HVjPEfViHAA36bqa7mqS9gduByyPijU7X00fSGcCmiFjd2Nxk1k7txy7gaOBrEXEU1XWOnT7d3U7pf50FzAAOACZQdSP62+k+rEOAank3U0njqMLzLxFxR2ne0R1Zd7UTgI9Jeha4jeo07nqqm/n3XSDcyf3YA/RExKoyvZQqUHXZfwAfBp6JiJcj4l3gDuB4hrgP6xCgh4GZ5duP8VQduTs7WZAkATcCayPi2oaXdnRH1l0qIq6KiGkRcTDV/vpuRFwAPACcU4P6XgQ2SjqsNM0BnqAm+6/YABwraa/y791X49D2Yac6cf06dKcBP6G6o+mf1qCeE6kO3T8C/rs8TmMHd2TtcK2zgeXl50OAHwJPAf8K7NHBuj4AdJd9+G1gUt32H3A18CTwOHAzsMdQ96Ev5TFLqMMpnNmI5QCZJThAZgkOkFmCA2SW4ACZJThAZgn/B6XWdJVxGzH6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the cart-pole environment\n",
    "env = gym.make('Tetris-v0')\n",
    "\n",
    "downsize = (82,108)\n",
    "\n",
    "# Reinitialize the environment for an episode\n",
    "observation = env.reset()\n",
    "observation = cv2.resize(observation, dsize=downsize, interpolation = cv2.INTER_CUBIC)\n",
    "# Look at the features the agent will observe during training...\n",
    "display(observation.shape)\n",
    "\n",
    "# Render the scene for our visualization purposes...\n",
    "#plt.imshow(env.render(mode='rgb_array'))\n",
    "#plt.title(\"New Episode\") \n",
    "#plt.show()\n",
    "plt.imshow(observation)\n",
    "plt.title(\"New Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 82, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shape of the observation vectors\n",
    "display(observation.shape)\n",
    "\n",
    "# Number of possible actions\n",
    "display(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize the environment and perform 10 random actions\n",
    "env.reset()\n",
    "reward = 0.0\n",
    "for time in range(100):\n",
    "    plt.imshow(cv2.resize(env.render(mode='rgb_array'),dsize=downsize,interpolation = cv2.INTER_CUBIC))\n",
    "    plt.title(\"%s | Time: %d | Reward: %f\" % (env.spec.id, time, reward))\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    observation, reward, done, info = env.step(np.random.choice(env.action_space.n)) # take a random action\n",
    "    if (done):\n",
    "        break\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 101, 32)       6176      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 68, 94, 64)        131136    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 34, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 34, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 102272)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2045460   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                252       \n",
      "=================================================================\n",
      "Total params: 2,183,024\n",
      "Trainable params: 2,183,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A simple multilayer architecture....\n",
    "def make_model(state, action_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size=(8, 8), activation='relu',\n",
    "                                    input_shape=[downsize[0],\n",
    "                                    downsize[1],\n",
    "                                    state.shape[2]]))\n",
    "    model.add(keras.layers.Conv2D(64, (8, 8), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "    return model\n",
    "\n",
    "model = make_model(cv2.resize(env.reset(), dsize=downsize, interpolation = cv2.INTER_CUBIC), env.action_space.n)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-18.382645 ,   2.7545547,  14.301205 ,  23.90409  ,  11.156364 ,\n",
       "        -16.15345  ,  19.17067  ,  26.44074  , -18.0085   , -10.319019 ,\n",
       "        -24.273376 , -10.147289 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our observation vector into a matrix of observations\n",
    "# with only -one- observation and run predict()\n",
    "Q = model.predict(np.expand_dims(cv2.resize(observation, dsize=(108,82), interpolation = cv2.INTER_CUBIC),axis=0))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example e-soft policy...\n",
    "epsilon = 0.5 # Half of the time, a random action is chosen...\n",
    "action = np.random.choice(env.action_space.n) if np.random.random() < epsilon else np.argmax(Q)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the environment and let the agent decide!\n",
    "observation = env.reset()\n",
    "observation = cv2.resize(observation, dsize=downsize, interpolation = cv2.INTER_CUBIC)\n",
    "epsilon = 1 # No exploration!\n",
    "reward = 0.0\n",
    "#for time in range(50):\n",
    "#    plt.imshow(env.render(mode='rgb_array'))\n",
    "#    plt.title(\"%s | Time: %d | Reward: %f\" % (env.spec.id, time, reward))\n",
    "#    clear_output(wait=True)\n",
    "#    display(plt.gcf())\n",
    "#    Q = model.predict(np.expand_dims(observation,axis=0)) # Compute Q\n",
    "#    action = np.random.choice(env.action_space.n) if np.random.random() < epsilon else np.argmax(Q)\n",
    "#    observation, reward, done, info = env.step(action) # take action!\n",
    "#    if (done):\n",
    "#        break\n",
    "#clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1 - Replay Memory Class\n",
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size, state_size, action_size):\n",
    "        self.state_size = [state_size.shape[0], state_size.shape[1], state_size.shape[2]]\n",
    "        self.action_size = action_size\n",
    "        self.size = 0\n",
    "        self.maxsize = memory_size\n",
    "        self.current_index = 0\n",
    "        self.current_state = np.zeros([memory_size, self.state_size[0], self.state_size[1], \n",
    "                                       self.state_size[2]])\n",
    "        self.action = [0]*memory_size # Remember, actions are integers...\n",
    "        self.reward = np.zeros([memory_size])\n",
    "        self.next_state = np.zeros([memory_size, self.state_size[0], self.state_size[1], \n",
    "                                       self.state_size[2]])\n",
    "        self.done = [False]*memory_size # Boolean (terminal transition?)\n",
    "        \n",
    "    def remember(self, current_state, action, reward, next_state, done):\n",
    "        # Stores a single memory item\n",
    "        self.current_state[self.current_index,:] = current_state\n",
    "        self.action[self.current_index] = action\n",
    "        self.reward[self.current_index] = reward\n",
    "        self.next_state[self.current_index,:] = next_state\n",
    "        self.done[self.current_index] = done\n",
    "        self.current_index = (self.current_index+1)%self.maxsize\n",
    "        self.size = max(self.current_index,self.size)\n",
    "    \n",
    "    def replay(self, model, target_model, num_samples, sample_size, gamma):\n",
    "        # Run replay!\n",
    "        \n",
    "        # Can't train if we don't yet have enough samples to begin with...\n",
    "        if self.size < sample_size:\n",
    "            return\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Select sample_size memory indices from the whole set\n",
    "            current_sample = np.random.choice(self.size,sample_size,replace=False)\n",
    "            \n",
    "            # Slice memory into training sample\n",
    "            current_state = self.current_state[current_sample,:]\n",
    "            action = [self.action[j] for j in current_sample]\n",
    "            reward = self.reward[current_sample]\n",
    "            next_state = self.next_state[current_sample,:]\n",
    "            done = [self.done[j] for j in current_sample]\n",
    "            \n",
    "            # Obtain model's current Q-values\n",
    "            #\n",
    "            #\n",
    "            # Here's the issue \n",
    "            # model_targets, targets, model.fit will require valid game state to make prediction\n",
    "            #\n",
    "            #\n",
    "            if current_state.size == 8856:\n",
    "                model_targets = model.predict(cv2.resize(current_state, dsize=(108,82), interpolation = cv2.INTER_CUBIC))\n",
    "            elif next_state.size == 8856:\n",
    "                model_targets = model.predict(cv2.resize(next_state, dsize=downsize, interpolation = cv2.INTER_CUBIC))\n",
    "            \n",
    "            # Create targets from argmax(Q(s+1,a+1))\n",
    "            # Use the target model!\n",
    "            if next_state.size == 8856:\n",
    "                targets = reward + gamma*np.amax(target_model.predict(cv2.resize(next_state, dsize=downsize, interpolation = cv2.INTER_CUBIC)),axis=1)\n",
    "            else:\n",
    "                targets = reward #This is an issue and definitely shouldn't be this\n",
    "            # Absorb the reward on terminal state-action transitions\n",
    "            targets[done] = reward[done]\n",
    "            # Update just the relevant parts of the model_target vector...\n",
    "            model_targets[range(sample_size),action] = targets\n",
    "            \n",
    "            # Update the weights accordingly\n",
    "            if current_state == 8856:\n",
    "                model.fit(cv2.resize(current_state, dsize=downsize, interpolation = cv2.INTER_CUBIC),model_targets,\n",
    "                         epochs=1,verbose=0,batch_size=sample_size)\n",
    "            \n",
    "        # Once we have finished training, update the target model\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Full setup for the task...\n",
    "\n",
    "# For keeping track of performance\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "epsilon_min = 0.01\n",
    "episodes = 1\n",
    "\n",
    "replay_iterations = 100\n",
    "replay_sample_size = 256\n",
    "\n",
    "# Peformance stats\n",
    "times_window = deque(maxlen=100)\n",
    "mean_times = deque(maxlen=episodes)\n",
    "\n",
    "# Initialize the environment and agent data structures\n",
    "env = gym.make('Tetris-v0')\n",
    "model = make_model(env.reset(), env.action_space.n)\n",
    "target_model = make_model(env.reset(), env.action_space.n)\n",
    "memory = ReplayMemory(700, env.reset(), env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1/1 - time: 999, mean-time: 999, epsilon: 0.990000"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model_targets' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-82afe1e887b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Training...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreplay_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreplay_sample_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-6cc213dd24e9>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, model, target_model, num_samples, sample_size, gamma)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;31m# Update just the relevant parts of the model_target vector...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mmodel_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# Update the weights accordingly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'model_targets' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform the training!\n",
    "for episode in range(episodes):\n",
    "    current_state = env.reset()\n",
    "    for time in range(1000):\n",
    "        Q = model.predict(np.expand_dims(cv2.resize(current_state, dsize=(108,82), interpolation = cv2.INTER_CUBIC), axis=0)) # Compute Q\n",
    "        action = np.random.choice(env.action_space.n) if np.random.random() < epsilon else np.argmax(Q)\n",
    "        next_state, reward, done, info = env.step(action) # take action!\n",
    "        if done:\n",
    "            reward = -10.0\n",
    "        memory.remember(current_state, action,reward,next_state,done) # Store in memory...\n",
    "        current_state = next_state # Transition to next state!\n",
    "        if (done):\n",
    "            break\n",
    "    epsilon = epsilon * epsilon_decay if epsilon > epsilon_min else epsilon_min\n",
    "    times_window.append(time)\n",
    "    mean_time = np.mean(times_window)\n",
    "    mean_times.append(mean_time)\n",
    "    print('\\rEpisode %d/%d - time: %d, mean-time: %d, epsilon: %f'%(episode+1,episodes,time,mean_time,epsilon),end='')\n",
    "    \n",
    "    # Training...\n",
    "    memory.replay(model,target_model,replay_iterations,replay_sample_size,gamma)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot results\n",
    "plt.plot(mean_times)\n",
    "plt.title(\"Cart-pole Q-Learning Performance\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Average # Steps [N=100]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test out the agent!\n",
    "# Initialize the environment and let the agent decide how to act!\n",
    "observation = env.reset()\n",
    "for time in range(50):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Time: %d | Reward: %f\" % (env.spec.id, time, reward))\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    Q = model.predict(np.expand_dims(observation,axis=0)) # Compute Q\n",
    "    action = np.argmax(Q)\n",
    "    observation, reward, done, info = env.step(action) # take action!\n",
    "    if (done):\n",
    "        break\n",
    "clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
